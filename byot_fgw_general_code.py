# -*- coding: utf-8 -*-
"""BYOT-FGW-General Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ynvNY8TCIIbZE86jMR9r4GKf1QWLIgGq

# **Policy**
"""

from PIL import Image, ImageEnhance, ImageOps
import numpy as np
import random


class ImageNetPolicy(object):
    """ Randomly choose one of the best 24 Sub-policies on ImageNet.

        Example:
        >>> policy = ImageNetPolicy()
        >>> transformed = policy(image)

        Example as a PyTorch Transform:
        >>> transform=transforms.Compose([
        >>>     transforms.Resize(256),
        >>>     ImageNetPolicy(),
        >>>     transforms.ToTensor()])
    """
    def __init__(self, fillcolor=(128, 128, 128)):
        self.policies = [
            SubPolicy(0.4, "posterize", 8, 0.6, "rotate", 9, fillcolor),
            SubPolicy(0.6, "solarize", 5, 0.6, "autocontrast", 5, fillcolor),
            SubPolicy(0.8, "equalize", 8, 0.6, "equalize", 3, fillcolor),
            SubPolicy(0.6, "posterize", 7, 0.6, "posterize", 6, fillcolor),
            SubPolicy(0.4, "equalize", 7, 0.2, "solarize", 4, fillcolor),

            SubPolicy(0.4, "equalize", 4, 0.8, "rotate", 8, fillcolor),
            SubPolicy(0.6, "solarize", 3, 0.6, "equalize", 7, fillcolor),
            SubPolicy(0.8, "posterize", 5, 1.0, "equalize", 2, fillcolor),
            SubPolicy(0.2, "rotate", 3, 0.6, "solarize", 8, fillcolor),
            SubPolicy(0.6, "equalize", 8, 0.4, "posterize", 6, fillcolor),

            SubPolicy(0.8, "rotate", 8, 0.4, "color", 0, fillcolor),
            SubPolicy(0.4, "rotate", 9, 0.6, "equalize", 2, fillcolor),
            SubPolicy(0.0, "equalize", 7, 0.8, "equalize", 8, fillcolor),
            SubPolicy(0.6, "invert", 4, 1.0, "equalize", 8, fillcolor),
            SubPolicy(0.6, "color", 4, 1.0, "contrast", 8, fillcolor),

            SubPolicy(0.8, "rotate", 8, 1.0, "color", 2, fillcolor),
            SubPolicy(0.8, "color", 8, 0.8, "solarize", 7, fillcolor),
            SubPolicy(0.4, "sharpness", 7, 0.6, "invert", 8, fillcolor),
            SubPolicy(0.6, "shearX", 5, 1.0, "equalize", 9, fillcolor),
            SubPolicy(0.4, "color", 0, 0.6, "equalize", 3, fillcolor),

            SubPolicy(0.4, "equalize", 7, 0.2, "solarize", 4, fillcolor),
            SubPolicy(0.6, "solarize", 5, 0.6, "autocontrast", 5, fillcolor),
            SubPolicy(0.6, "invert", 4, 1.0, "equalize", 8, fillcolor),
            SubPolicy(0.6, "color", 4, 1.0, "contrast", 8, fillcolor),
            SubPolicy(0.8, "equalize", 8, 0.6, "equalize", 3, fillcolor)
        ]


    def __call__(self, img):
        policy_idx = random.randint(0, len(self.policies) - 1)
        return self.policies[policy_idx](img)

    def __repr__(self):
        return "AutoAugment ImageNet Policy"


class CIFAR10Policy(object):
    """ Randomly choose one of the best 25 Sub-policies on CIFAR10.

        Example:
        >>> policy = CIFAR10Policy()
        >>> transformed = policy(image)

        Example as a PyTorch Transform:
        >>> transform=transforms.Compose([
        >>>     transforms.Resize(256),
        >>>     CIFAR10Policy(),
        >>>     transforms.ToTensor()])
    """
    def __init__(self, fillcolor=(128, 128, 128)):
        self.policies = [
            SubPolicy(0.1, "invert", 7, 0.2, "contrast", 6, fillcolor),
            SubPolicy(0.7, "rotate", 2, 0.3, "translateX", 9, fillcolor),
            SubPolicy(0.8, "sharpness", 1, 0.9, "sharpness", 3, fillcolor),
            SubPolicy(0.5, "shearY", 8, 0.7, "translateY", 9, fillcolor),
            SubPolicy(0.5, "autocontrast", 8, 0.9, "equalize", 2, fillcolor),

            SubPolicy(0.2, "shearY", 7, 0.3, "posterize", 7, fillcolor),
            SubPolicy(0.4, "color", 3, 0.6, "brightness", 7, fillcolor),
            SubPolicy(0.3, "sharpness", 9, 0.7, "brightness", 9, fillcolor),
            SubPolicy(0.6, "equalize", 5, 0.5, "equalize", 1, fillcolor),
            SubPolicy(0.6, "contrast", 7, 0.6, "sharpness", 5, fillcolor),

            SubPolicy(0.7, "color", 7, 0.5, "translateX", 8, fillcolor),
            SubPolicy(0.3, "equalize", 7, 0.4, "autocontrast", 8, fillcolor),
            SubPolicy(0.4, "translateY", 3, 0.2, "sharpness", 6, fillcolor),
            SubPolicy(0.9, "brightness", 6, 0.2, "color", 8, fillcolor),
            SubPolicy(0.5, "solarize", 2, 0.0, "invert", 3, fillcolor),

            SubPolicy(0.2, "equalize", 0, 0.6, "autocontrast", 0, fillcolor),
            SubPolicy(0.2, "equalize", 8, 0.6, "equalize", 4, fillcolor),
            SubPolicy(0.9, "color", 9, 0.6, "equalize", 6, fillcolor),
            SubPolicy(0.8, "autocontrast", 4, 0.2, "solarize", 8, fillcolor),
            SubPolicy(0.1, "brightness", 3, 0.7, "color", 0, fillcolor),

            SubPolicy(0.4, "solarize", 5, 0.9, "autocontrast", 3, fillcolor),
            SubPolicy(0.9, "translateY", 9, 0.7, "translateY", 9, fillcolor),
            SubPolicy(0.9, "autocontrast", 2, 0.8, "solarize", 3, fillcolor),
            SubPolicy(0.8, "equalize", 8, 0.1, "invert", 3, fillcolor),
            SubPolicy(0.7, "translateY", 9, 0.9, "autocontrast", 1, fillcolor)
        ]


    def __call__(self, img):
        policy_idx = random.randint(0, len(self.policies) - 1)
        return self.policies[policy_idx](img)

    def __repr__(self):
        return "AutoAugment CIFAR10 Policy"


class SVHNPolicy(object):
    """ Randomly choose one of the best 25 Sub-policies on SVHN.

        Example:
        >>> policy = SVHNPolicy()
        >>> transformed = policy(image)

        Example as a PyTorch Transform:
        >>> transform=transforms.Compose([
        >>>     transforms.Resize(256),
        >>>     SVHNPolicy(),
        >>>     transforms.ToTensor()])
    """
    def __init__(self, fillcolor=(128, 128, 128)):
        self.policies = [
            SubPolicy(0.9, "shearX", 4, 0.2, "invert", 3, fillcolor),
            SubPolicy(0.9, "shearY", 8, 0.7, "invert", 5, fillcolor),
            SubPolicy(0.6, "equalize", 5, 0.6, "solarize", 6, fillcolor),
            SubPolicy(0.9, "invert", 3, 0.6, "equalize", 3, fillcolor),
            SubPolicy(0.6, "equalize", 1, 0.9, "rotate", 3, fillcolor),

            SubPolicy(0.9, "shearX", 4, 0.8, "autocontrast", 3, fillcolor),
            SubPolicy(0.9, "shearY", 8, 0.4, "invert", 5, fillcolor),
            SubPolicy(0.9, "shearY", 5, 0.2, "solarize", 6, fillcolor),
            SubPolicy(0.9, "invert", 6, 0.8, "autocontrast", 1, fillcolor),
            SubPolicy(0.6, "equalize", 3, 0.9, "rotate", 3, fillcolor),

            SubPolicy(0.9, "shearX", 4, 0.3, "solarize", 3, fillcolor),
            SubPolicy(0.8, "shearY", 8, 0.7, "invert", 4, fillcolor),
            SubPolicy(0.9, "equalize", 5, 0.6, "translateY", 6, fillcolor),
            SubPolicy(0.9, "invert", 4, 0.6, "equalize", 7, fillcolor),
            SubPolicy(0.3, "contrast", 3, 0.8, "rotate", 4, fillcolor),

            SubPolicy(0.8, "invert", 5, 0.0, "translateY", 2, fillcolor),
            SubPolicy(0.7, "shearY", 6, 0.4, "solarize", 8, fillcolor),
            SubPolicy(0.6, "invert", 4, 0.8, "rotate", 4, fillcolor),
            SubPolicy(0.3, "shearY", 7, 0.9, "translateX", 3, fillcolor),
            SubPolicy(0.1, "shearX", 6, 0.6, "invert", 5, fillcolor),

            SubPolicy(0.7, "solarize", 2, 0.6, "translateY", 7, fillcolor),
            SubPolicy(0.8, "shearY", 4, 0.8, "invert", 8, fillcolor),
            SubPolicy(0.7, "shearX", 9, 0.8, "translateY", 3, fillcolor),
            SubPolicy(0.8, "shearY", 5, 0.7, "autocontrast", 3, fillcolor),
            SubPolicy(0.7, "shearX", 2, 0.1, "invert", 5, fillcolor)
        ]


    def __call__(self, img):
        policy_idx = random.randint(0, len(self.policies) - 1)
        return self.policies[policy_idx](img)

    def __repr__(self):
        return "AutoAugment SVHN Policy"


class SubPolicy(object):
    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):
        ranges = {
            "shearX": np.linspace(0, 0.3, 10),
            "shearY": np.linspace(0, 0.3, 10),
            "translateX": np.linspace(0, 150 / 331, 10),
            "translateY": np.linspace(0, 150 / 331, 10),
            "rotate": np.linspace(0, 30, 10),
            "color": np.linspace(0.0, 0.9, 10),
            "posterize": np.round(np.linspace(8, 4, 10), 0).astype(int),
            "solarize": np.linspace(256, 0, 10),
            "contrast": np.linspace(0.0, 0.9, 10),
            "sharpness": np.linspace(0.0, 0.9, 10),
            "brightness": np.linspace(0.0, 0.9, 10),
            "autocontrast": [0] * 10,
            "equalize": [0] * 10,
            "invert": [0] * 10
        }

        # from https://stackoverflow.com/questions/5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand
        def rotate_with_fill(img, magnitude):
            rot = img.convert("RGBA").rotate(magnitude)
            return Image.composite(rot, Image.new("RGBA", rot.size, (128,) * 4), rot).convert(img.mode)

        func = {
            "shearX": lambda img, magnitude: img.transform(
                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),
                Image.BICUBIC, fillcolor=fillcolor),
            "shearY": lambda img, magnitude: img.transform(
                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),
                Image.BICUBIC, fillcolor=fillcolor),
            "translateX": lambda img, magnitude: img.transform(
                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),
                fillcolor=fillcolor),
            "translateY": lambda img, magnitude: img.transform(
                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),
                fillcolor=fillcolor),
            "rotate": lambda img, magnitude: rotate_with_fill(img, magnitude),
            "color": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),
            "posterize": lambda img, magnitude: ImageOps.posterize(img, magnitude),
            "solarize": lambda img, magnitude: ImageOps.solarize(img, magnitude),
            "contrast": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(
                1 + magnitude * random.choice([-1, 1])),
            "sharpness": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(
                1 + magnitude * random.choice([-1, 1])),
            "brightness": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(
                1 + magnitude * random.choice([-1, 1])),
            "autocontrast": lambda img, magnitude: ImageOps.autocontrast(img),
            "equalize": lambda img, magnitude: ImageOps.equalize(img),
            "invert": lambda img, magnitude: ImageOps.invert(img)
        }

        self.p1 = p1
        self.operation1 = func[operation1]
        self.magnitude1 = ranges[operation1][magnitude_idx1]
        self.p2 = p2
        self.operation2 = func[operation2]
        self.magnitude2 = ranges[operation2][magnitude_idx2]


    def __call__(self, img):
        if random.random() < self.p1: img = self.operation1(img, self.magnitude1)
        if random.random() < self.p2: img = self.operation2(img, self.magnitude2)
        return img

import torch
import numpy as np


class Cutout(object):
    """Randomly mask out one or more patches from an image.

    Args:
        n_holes (int): Number of patches to cut out of each image.
        length (int): The length (in pixels) of each square patch.
    """
    def __init__(self, n_holes, length):
        self.n_holes = n_holes
        self.length = length

    def __call__(self, img):
        """
        Args:
            img (Tensor): Tensor image of size (C, H, W).
        Returns:
            Tensor: Image with n_holes of dimension length x length cut out of it.
        """
        h = img.size(1)
        w = img.size(2)

        mask = np.ones((h, w), np.float32)

        for n in range(self.n_holes):
            y = np.random.randint(h)
            x = np.random.randint(w)

            y1 = np.clip(y - self.length // 2, 0, h)
            y2 = np.clip(y + self.length // 2, 0, h)
            x1 = np.clip(x - self.length // 2, 0, w)
            x2 = np.clip(x + self.length // 2, 0, w)

            mask[y1: y2, x1: x2] = 0.

        mask = torch.from_numpy(mask)
        mask = mask.expand_as(img)
        img = img * mask

        return img

"""# **FGW model**"""

import torch
import torch.nn as nn
import torch.nn.functional as F

# ================================== Channel Attention Map ===================================
def logsumexp_2d(tensor):
    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)
    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)
    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()
    return outputs

class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)

class CAM(nn.Module):

    def __init__(self, in_channels, reduction_ratio=16, pool_types=['avg', 'max']):
        super(CAM, self).__init__()
        self.in_channels = in_channels
        self.mlp = nn.Sequential(
            Flatten(),  # (b x c)
            nn.Linear(in_channels, in_channels //
                      reduction_ratio),  # (b x (c/r))
            nn.Dropout(0.5),
            nn.ReLU(inplace=True),  # (b x (c/r))
            nn.Linear(in_channels // reduction_ratio, in_channels),  # (b x c)
            nn.Dropout(0.5),
            nn.ReLU(inplace=True)
        )
        self.pool_types = pool_types

    def forward(self, x):
        # x: (b x c x h x w)
        channel_att_sum = None
        for pool_type in self.pool_types:
            if pool_type == 'avg':
                avg_pool = F.avg_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))  # (b x c x 1 x 1)
                channel_att_raw = self.mlp(avg_pool) # (b x c)
            elif pool_type == 'max':
                max_pool = F.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3))) # (b x c x 1 x 1)
                channel_att_raw = self.mlp(max_pool) # (b x c)
            elif pool_type == 'lp':
                lp_pool = F.lp_pool2d(x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))
                channel_att_raw = self.mlp(lp_pool)
            elif pool_type == 'lse':
                lse_pool = logsumexp_2d(x)
                channel_att_raw = self.mlp(lse_pool)

            if channel_att_sum is None:
                channel_att_sum = channel_att_raw # (b x c)
            else:
                channel_att_sum = channel_att_sum + channel_att_raw # (b x c)

        # print(channel_att_sum.unsqueeze(2).unsqueeze(3).shape) # (b x c x 1 x 1)
        scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x) # (b x c x h x w)
        return x * scale


class CBAM(nn.Module):
    def __init__(self, in_channels, reduction_ratio=16, pool_types=['avg', 'max'], is_spatial=True):
        super(CBAM, self).__init__()
        self.cam = CAM(in_channels, reduction_ratio, pool_types)
        self.is_spatial = is_spatial
        if is_spatial:
            self.sam = SAM()

    def forward(self, x):
        out = self.cam(x)
        if self.is_spatial:
            out = self.sam(out)
        return out



# ================================== Spatial Attention Map ===================================
class ConvLayer(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,
                 padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):
        super(ConvLayer, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)
        self.bn = nn.BatchNorm2d(out_channels, affine=True, momentum=0.99, eps=1e-3) if bn else None
        self.relu = nn.ReLU(inplace=True) if relu else None

    def forward(self, x):
        x = self.conv(x)
        if self.bn is not None:
            x = self.bn(x)
        if self.relu is not None:
            x = self.relu(x)
        return x

class ChannelPooling(nn.Module):
    def forward(self, x):
        return torch.cat((torch.max(x, 1)[0].unsqueeze(1), torch.mean(x, 1).unsqueeze(1)), dim=1)

class SAM(nn.Module):
    def __init__(self, kernel_size=7):
        super(SAM, self).__init__()
        self.pool = ChannelPooling()
        self.conv = ConvLayer(2, 1, kernel_size, stride=1, padding=(kernel_size-1)//2, relu=False)

    def forward(self, x):
        out = self.pool(x)
        out = self.conv(out)
        scale = torch.sigmoid(out)
        return scale * x

import torch
import torch.nn as nn

def conv3x3(in_channels, out_channels, kernel_size=3, stride=1, padding=1):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),
        nn.BatchNorm2d(out_channels, affine=True, momentum=0.99, eps=1e-3),
        nn.ReLU(inplace=True)
    )

class SeparableConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):
        super(SeparableConv2d, self).__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=1, padding=padding, dilation=dilation, groups=in_channels,
                                    bias=bias)
        self.bnd = nn.BatchNorm2d(in_channels, affine=True)
        self.relu = nn.ReLU(inplace=True)
        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, stride, 0, 1, 1, bias=bias)
        self.bnp = nn.BatchNorm2d(out_channels, affine=True)

    def forward(self, x):
        out = self.depthwise(x)
        out = self.relu(self.bnd(out))
        out = self.pointwise(out)
        out = self.relu(self.bnp(out))
        return out

class Block(nn.Module):
    def __init__(self, in_channels, out_channels, keep_dim=False):
        super(Block, self).__init__()

        self.keep_dim = keep_dim
        stride_sep_conv2 = 2
        if keep_dim:
            stride_sep_conv2 = 1
        self.conv = conv3x3(in_channels, out_channels, kernel_size=1, stride=1, padding=0)
        self.sep_conv1 = SeparableConv2d(in_channels, out_channels, kernel_size=3, bias=False, padding=1)
        self.sep_conv2 = SeparableConv2d(out_channels, out_channels, kernel_size=3, stride=stride_sep_conv2, bias=False, padding=1)
        self.cbam = CBAM(out_channels)
        self.maxp = nn.MaxPool2d(kernel_size=2, stride=2) if not keep_dim else nn.Identity()
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        residual = self.conv(x)
        if not self.keep_dim:
            residual = self.maxp(residual)

        out = self.sep_conv1(x)
        out = self.sep_conv2(out)
        out = self.cbam(out)
        out += residual

        return out

class Model(nn.Module):
    def __init__(self, in_channels, num_classes):
        super(Model, self).__init__()
        self.relu = nn.ReLU(inplace=True)
        # 1
        self.conv1 = conv3x3(in_channels, 8)
        self.conv2 = conv3x3(8, 8)
        self.cbam = CBAM(8,8)

        # 2
        self.block1 = Block(8, 16, keep_dim=True)

        # 3
        self.block2 = Block(16, 32, keep_dim=True)

        # 4
        self.block3 = Block(32, 64, keep_dim=False)

        # 5
        self.block4 = Block(64, 128, keep_dim=False)

        # last conv to down to num_classes
        self.last_conv = conv3x3(128, num_classes)

        # global avg-p
        self.avgp = nn.AdaptiveAvgPool2d((1, 1))

    def forward(self, x):
        # Initial convolutions
        out = self.conv1(x)
        out = self.conv2(out)
        out = self.cbam(out)

        # Block processing
        out = self.block1(out)
        out = self.block2(out)
        out = self.block3(out)
        out = self.block4(out)

        # Final convolution and pooling
        out = self.last_conv(out)
        out = self.avgp(out)
        out = out.view((out.shape[0], -1))

        return out


if __name__ == "__main__":
    import time
    import numpy as np
    model = Model(64, 100)
    print('total params: ', sum(p.numel() for p in model.parameters() if p.requires_grad))
    model.eval()
    x = torch.rand(128, 64, 32, 32)
    t0 = time.time()
    out = model(x)
    t = time.time() - t0
    print('time inference: ', t)

"""# **Model**"""

# Arguments
args = {
    'model': 'resnet18',
    'dataset': 'fer2013',
    'epoch': 120, # Default BYOT is 250
    'loss_coefficient': 0.3,
    'feature_loss_coefficient': 0.03,
    'dataset_path': 'data',
    'temperature': 3.0,
    'batchsize': 256,
    'init_lr': 0.1,
    'use_wandb': True
}

if args['dataset'] == 'cifar10':
    data_num_classes = 10
elif args['dataset'] == 'cifar100':
    data_num_classes = 100
elif args['dataset'] in ['fer2013', 'ferplus']:
    data_num_classes = 7
elif args['dataset'] == 'imagenet':
    data_num_classes = 1000

import torch
import torch.nn as nn
import torch.nn.functional as F

def conv_3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=dilation, groups=groups, bias=False, dilation=dilation)


def conv_1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


class BasicBlock(nn.Module):
    expansion = 1
    __constants__ = ['downsample']

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None):
        super(BasicBlock, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError('BasicBlock only supports groups=1 and base_width=64')
        if dilation > 1:
            raise NotImplementedError("Dilation > 1 not supported in BasicBlock")
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv_3x3(inplanes, planes, stride)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv_3x3(planes, planes)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out

class Bottleneck(nn.Module):
    expansion = 4
    __constants__ = ['downsample']

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None):
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.)) * groups
        # Both self.conv2 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv_1x1(inplanes, width)
        self.bn1 = norm_layer(width)
        self.conv2 = conv_3x3(width, width, stride, groups, dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = conv_1x1(width, planes * self.expansion)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out



class SepConv(nn.Module):

    def __init__(self, channel_in, channel_out, kernel_size=3, stride=2, padding=1, affine=True):
        super(SepConv, self).__init__()
        self.op = nn.Sequential(
            nn.Conv2d(channel_in, channel_in, kernel_size=kernel_size, stride=stride, padding=padding, groups=channel_in, bias=False),
            nn.Conv2d(channel_in, channel_in, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(channel_in, affine=affine),
            nn.ReLU(inplace=False),
            nn.Conv2d(channel_in, channel_in, kernel_size=kernel_size, stride=1, padding=padding, groups=channel_in, bias=False),
            nn.Conv2d(channel_in, channel_out, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(channel_out, affine=affine),
            nn.ReLU(inplace=False),
        )

    def forward(self, x):
        return self.op(x)



class ResNet(nn.Module):

    def __init__(self, block, layers, num_classes, zero_init_residual=False,
                 groups=1, width_per_group=64, replace_stride_with_dilation=None,
                 norm_layer=None):
        super(ResNet, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        self._norm_layer = norm_layer
        self.inplanes = 64

        self.dilation = 1
        if replace_stride_with_dilation is None:
            # each element in the tuple indicates if we should replace
            # the 2x2 stride with a dilated convolution instead
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError("replace_stride_with_dilation should be None "
                             "or a 3-element tuple, got {}".format(replace_stride_with_dilation))
        self.groups = groups
        self.base_width = width_per_group

        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1,bias=False)
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,
                                       dilate=replace_stride_with_dilation[0])
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,
                                       dilate=replace_stride_with_dilation[1])
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,
                                       dilate=replace_stride_with_dilation[2])

        self.expans = block.expansion
        self.cbam1 = CBAM(64*self.expans, 64*self.expans)
        self.cbam2 = CBAM(128*self.expans, 128*self.expans)
        self.cbam3 = CBAM(256*self.expans, 256*self.expans)
        # last conv to down to num_classes
        self.last_conv = conv3x3(512*self.expans, num_classes)
        #self.last_conv = nn.Linear(512*self.expans, num_classes)
        # global avg-pooling
        self.avgp = nn.AdaptiveAvgPool2d((1, 1))


        self.block1_fgw = nn.Sequential(Block(64*self.expans, 128*self.expans),
                                        Block(128*self.expans, 256*self.expans),
                                        Block(256*self.expans, 512*self.expans, keep_dim=False))
        self.block2_fgw = nn.Sequential(Block(128*self.expans, 256*self.expans),
                                        Block(256*self.expans, 512*self.expans, keep_dim=False))
        self.block3_fgw = nn.Sequential(Block(256*self.expans, 512*self.expans, keep_dim=False))

        self.fc = nn.Linear(512*self.expans, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677

    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):
        norm_layer = self._norm_layer
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                conv_1x1(self.inplanes, planes * block.expansion, stride),
                norm_layer(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,
                            self.base_width, previous_dilation, norm_layer))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, groups=self.groups,
                                base_width=self.base_width, dilation=self.dilation,
                                norm_layer=norm_layer))

        return nn.Sequential(*layers)


    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)

        x = self.layer1(x)
        out1 = x
        out1 = self.cbam1(out1)
        out1 = self.block1_fgw(out1)
        out1 = self.avgp(out1)
        out1 = out1.view((out1.shape[0], -1))
        fea1 = out1
        out1 = self.fc(out1)

        x = self.layer2(x)
        out2 = x
        out2 = self.cbam2(out2)
        out2 = self.block2_fgw(out2)
        out2 = self.avgp(out2)
        out2 = out2.view((out2.shape[0], -1))
        fea2 = out2
        out2 = self.fc(out2)

        x = self.layer3(x)
        out3 = x
        out3 = self.cbam3(out3)
        out3 = self.block3_fgw(out3)
        out3 = self.avgp(out3)
        out3 = out3.view((out3.shape[0], -1))
        fea3 = out3
        out3 = self.fc(out3)

        x = self.layer4(x)
        out4 = x
        out4 = self.avgp(out4)
        out4 = out4.view((out4.shape[0], -1))
        fea4 = out4
        out4 = self.fc(out4)

        return [out4, out3, out2, out1], [fea4, fea3, fea2, fea1]

def _resnet(arch, block, layers, pretrained, progress, **kwargs):
    model = ResNet(block, layers, num_classes=data_num_classes, **kwargs)
    if pretrained:
        state_dict = load_state_dict_from_url(model_urls[arch],
                                            progress=progress)
        model.load_state_dict(state_dict)
    return model


def resnet18(pretrained=False, progress=True, **kwargs):
    r"""ResNet-18 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,
                   **kwargs)

def resnet34(pretrained=False, progress=True, **kwargs):
    r"""ResNet-34 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)

def resnet50(pretrained=False, progress=True, **kwargs):
    r"""ResNet-50 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)



if __name__ == "__main__":
    import numpy as np
    model = resnet18()
    print('Total params: M', sum(p.numel() for p in model.parameters()) / 1e6)
    model.eval()

if __name__ == '__main__':

    model = resnet18(pretrained=False)

    param_counts = {
        'conv1': sum(p.numel() for p in model.conv1.parameters()),
        'bn1': sum(p.numel() for p in model.bn1.parameters()),

        'layer1': sum(p.numel() for p in model.layer1.parameters()),
        'cbam1': sum(p.numel() for p in model.cbam1.parameters()),
        'block1_fgw': sum(p.numel() for p in model.block1_fgw.parameters()),
        'fc1': sum(p.numel() for p in model.fc.parameters()),

        'layer2': sum(p.numel() for p in model.layer2.parameters()),
        'cbam2': sum(p.numel() for p in model.cbam2.parameters()),
        'block2_fgw': sum(p.numel() for p in model.block2_fgw.parameters()),
        'fc2': sum(p.numel() for p in model.fc.parameters()),

        'layer3': sum(p.numel() for p in model.layer3.parameters()),
        'cbam3': sum(p.numel() for p in model.cbam3.parameters()),
        'block3_fgw': sum(p.numel() for p in model.block3_fgw.parameters()),
        'fc3': sum(p.numel() for p in model.fc.parameters()),

        'layer4': sum(p.numel() for p in model.layer4.parameters()),
        'fc4': sum(p.numel() for p in model.fc.parameters()),
    }

    branch_params = {
        'branch1': param_counts['conv1'] + param_counts['bn1'] + param_counts['layer1'] + param_counts['cbam1'] + param_counts['block1_fgw'] + param_counts['fc1'],
        'branch2': param_counts['conv1'] + param_counts['bn1'] + param_counts['layer1'] + param_counts['layer2'] + param_counts['cbam2'] + param_counts['block2_fgw'] + param_counts['fc2'],
        'branch3': param_counts['conv1'] + param_counts['bn1'] + param_counts['layer1'] + param_counts['layer2'] + param_counts['layer3'] + param_counts['cbam3'] + param_counts['block3_fgw'] + param_counts['fc3'],
        'branch4': param_counts['conv1'] + param_counts['bn1'] + param_counts['layer1'] + param_counts['layer2'] + param_counts['layer3'] + param_counts['layer4'] + param_counts['fc4'],
    }

    import torchvision
    resnet18_vi = torchvision.models.resnet18()

    print('resnet18_torchvision params : M', sum(p.numel() for p in resnet18_vi.parameters()) / 1e6)
    print('model params : M', sum(p.numel() for p in model.parameters()) / 1e6)

    for branch, count in branch_params.items():
        print(f'{branch}_params: {count / 1e6}M')

# Run test the model with sample data
if __name__ == "__main__":
    import numpy as np
    model = resnet18()
    x = torch.rand(1, 3, 48, 48)
    out = model(x)
    print(out[1][0].size())

"""# **Data**"""

import subprocess
import os
import shutil

def install_package(package):
    subprocess.check_call(["pip", "install", package])

def download_file(file_id, output_name):
    install_package("gdown")
    import gdown
    gdown.download(id=file_id, output=output_name, quiet=False)

def unzip_file(zip_file):
    import zipfile
    with zipfile.ZipFile(zip_file, 'r') as zip_ref:
        zip_ref.extractall()
    os.remove(zip_file)

def remove_directory(directory):
    if os.path.exists(directory):
        shutil.rmtree(directory)


if args['dataset'] == "fer2013":
    download_file('1YBuZaO7morIG43trYi0qtdelYGukBNCj', 'FER2013.zip')
    unzip_file('FER2013.zip')
elif args['dataset'] == "ferplus":
    download_file('1LShk6tZlsdBO-DciChK7y7nivUOvTAFk', 'FERPlus.zip')
    unzip_file('FERPlus.zip')
    remove_directory('fer_plus/train/contempt')
    remove_directory('fer_plus/val/contempt')
    remove_directory('fer_plus/test/contempt')

import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torch

# Data augmentation
if args['dataset'] in ['fer2013', 'ferplus']:
  target_size = 48
elif args['dataset'] in ['cifar10', 'cifar100']:
  target_size = 32

mean = 0
std = 255

data_transforms_CIFAR = {
    'train': transforms.Compose([
        transforms.RandomCrop(32, padding=4, fill=128),
        transforms.RandomHorizontalFlip(), CIFAR10Policy(), transforms.ToTensor(),
        Cutout(n_holes=1, length=16),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ]),
    'val': transforms.Compose([
        transforms.RandomCrop(32, padding=4, fill=128),
        transforms.RandomHorizontalFlip(), transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ]),
    'test': transforms.Compose([
        transforms.RandomCrop(32, padding=4, fill=128),
        transforms.RandomHorizontalFlip(), transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
}

data_transforms_FER = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(target_size, scale=(0.8, 1.2)),
        transforms.RandomApply([transforms.RandomAffine(0, translate=(0.2, 0.2))], p=0.5),
        transforms.RandomHorizontalFlip(),
        transforms.RandomApply([transforms.RandomRotation(10)], p=0.5),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std)
    ]),
    'val': transforms.Compose([
        transforms.Resize((target_size, target_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std)
    ]),
    'test': transforms.Compose([
        transforms.Resize((target_size, target_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std)
    ])
}

if args['dataset'] == "cifar100":
    trainset = datasets.CIFAR100(
        root=args['dataset_path'],
        train=True,
        download=True,
        transform=data_transforms_CIFAR['train']
    )
    valset = datasets.CIFAR100(
        root=args['dataset_path'],
        train=False,
        download=True,
        transform=data_transforms_CIFAR['val']
    )
    testset = datasets.CIFAR100(
        root=args['dataset_path'],
        train=False,
        download=True,
        transform=data_transforms_CIFAR['test']
    )
elif args['dataset'] == "cifar10":
    trainset = datasets.CIFAR10(
        root=args['dataset_path'],
        train=True,
        download=True,
        transform=data_transforms_CIFAR['train']
    )
    valset = datasets.CIFAR10(
        root=args['dataset_path'],
        train=False,
        download=True,
        transform=data_transforms_CIFAR['val']
    )
    testset = datasets.CIFAR10(
        root=args['dataset_path'],
        train=False,
        download=True,
        transform=data_transforms_CIFAR['test']
    )
elif args['dataset'] == "fer2013":
    trainset = datasets.ImageFolder(
        root='org_fer2013/train',
        transform=data_transforms_FER['train']
    )
    valset = datasets.ImageFolder(
        root='org_fer2013/val',
        transform=data_transforms_FER['val']
    )
    testset = datasets.ImageFolder(
        root='org_fer2013/test',
        transform=data_transforms_FER['test']
    )
elif args['dataset'] == "ferplus":
    trainset = datasets.ImageFolder(
        root='fer_plus/train',
        transform=data_transforms_FER['train']
    )
    valset = datasets.ImageFolder(
        root='fer_plus/val',
        transform=data_transforms_FER['val']
    )
    testset = datasets.ImageFolder(
        root='fer_plus/test',
        transform=data_transforms_FER['test']
    )


trainloader = torch.utils.data.DataLoader(
    trainset,
    batch_size=args['batchsize'],
    shuffle=True,
    num_workers=4
)
valloader = torch.utils.data.DataLoader(
    valset,
    batch_size=args['batchsize'],
    shuffle=False,
    num_workers=4
)
testloader = torch.utils.data.DataLoader(
    testset,
    batch_size=args['batchsize'],
    shuffle=False,
    num_workers=4
)

if args['model'] == "resnet18":
    net = resnet18()
if args['model'] == "resnet34":
    net = resnet34()
if args['model'] == "resnet50":
    net = resnet50()
if args['model'] == "resnet101":
    net = resnet101()
if args['model'] == "resnet152":
    net = resnet152()
if args['model'] == "wideresnet50":
    net = wide_resnet50_2()
if args['model'] == "wideresnet101":
    net = wide_resnet101_2()
if args['model'] == "resnext50_32x4d":
    net = resnet18()
if args['model'] == "resnext101_32x8d":
    net = resnext101_32x8d()

if args['dataset'] in ['fer2013', 'ferplus']:
    import matplotlib.pyplot as plt
    import seaborn as sns
    from collections import Counter

    # Function to get class distribution
    def get_class_distribution(dataset):
        if isinstance(dataset, datasets.ImageFolder):
            classes = [item[1] for item in dataset.imgs]
            class_to_idx = dataset.class_to_idx
            idx_to_class = {v: k for k, v in class_to_idx.items()}
        else:
            classes = dataset.targets
            idx_to_class = {i: cls for i, cls in enumerate(dataset.classes)}
        return classes, idx_to_class

    # Extracting class distributions and class names
    train_classes, train_idx_to_class = get_class_distribution(trainset)
    val_classes, val_idx_to_class = get_class_distribution(valset)
    test_classes, test_idx_to_class = get_class_distribution(testset)

    # Counting occurrences of each class
    train_class_count = Counter(train_classes)
    val_class_count = Counter(val_classes)
    test_class_count = Counter(test_classes)

    # Sorting class counts by class label
    sorted_train_class_count = dict(sorted(train_class_count.items()))
    sorted_val_class_count = dict(sorted(val_class_count.items()))
    sorted_test_class_count = dict(sorted(test_class_count.items()))

    # Mapping sorted class indices to class names
    train_class_labels = [train_idx_to_class[idx] for idx in sorted_train_class_count.keys()]
    val_class_labels = [val_idx_to_class[idx] for idx in sorted_val_class_count.keys()]
    test_class_labels = [test_idx_to_class[idx] for idx in sorted_test_class_count.keys()]

    # Plotting the distribution
    fig, axes = plt.subplots(1, 3, figsize=(30, 8))
    sns.barplot(x=train_class_labels, y=list(sorted_train_class_count.values()), ax=axes[0])
    sns.barplot(x=val_class_labels, y=list(sorted_val_class_count.values()), ax=axes[1])
    sns.barplot(x=test_class_labels, y=list(sorted_test_class_count.values()), ax=axes[2])

    # Adding text annotations
    for p in axes[0].patches:
        axes[0].annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                        ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')
    for p in axes[1].patches:
        axes[1].annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                        ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')
    for p in axes[2].patches:
        axes[2].annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                        ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')

    # Setting titles and labels
    if args["dataset"] == "fer2013":
        fig.suptitle('FER2013 Dataset Distribution', fontsize=16)
    elif args["dataset"] == "ferplus":
        fig.suptitle('FERPlus Dataset Distribution', fontsize=16)

    axes[0].set_title('Train Dataset Class Distribution')
    axes[0].set_xlabel('Class Labels')
    axes[0].set_ylabel('Samples')
    axes[0].tick_params(axis='x')

    axes[1].set_title('Validation Dataset Class Distribution')
    axes[1].set_xlabel('Class Labels')
    axes[1].set_ylabel('Samples')
    axes[1].tick_params(axis='x')

    axes[2].set_title('Test Dataset Class Distribution')
    axes[2].set_xlabel('Class Labels')
    axes[2].set_ylabel('Samples')
    axes[2].tick_params(axis='x')

    plt.tight_layout(rect=[0, 0, 1, 0.98])  # Adjust layout to make room for the suptitle
    plt.show()

"""# **Train**"""

use_wandb = args['use_wandb']
if use_wandb:
    install_package('wandb')
    import wandb
    wandb.login()

import torch.nn as nn
import torch.optim as optim
import torchvision
import argparse
import os
from tqdm.notebook import tqdm
import torch.nn.functional as F
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(args)

if use_wandb:
    wandb.init(
        project = "BYOT-FER",
        config = args
    )

def CrossEntropy(outputs, targets):
    log_softmax_outputs = F.log_softmax(outputs/args['temperature'], dim=1)
    softmax_targets = F.softmax(targets/args['temperature'], dim=1)
    return -(log_softmax_outputs * softmax_targets).sum(dim=1).mean()


#net = nn.DataParallel(net, device_ids=[0, 1]).to(device)
net.to(device)

criterion = nn.CrossEntropyLoss()
if args["dataset"] in ["fer2013", "ferplus"]:
    optimizer = optim.AdamW(net.parameters(), lr=args['init_lr'], weight_decay=5e-4)
elif args["dataset"] in ["cifar10", "cifar100"]:
    optimizer = optim.SGD(net.parameters(), lr=args['init_lr'], weight_decay=5e-4, momentum=0.9)

init = False


if __name__ == "__main__":
    best_acc = 0
    if not os.path.exists("./checkpoints"):
        os.makedirs("./checkpoints")
    for epoch in range(args['epoch']):
        correct = [0 for _ in range(5)]
        predicted = [0 for _ in range(5)]
        if epoch in [args['epoch'] // 3, args['epoch'] * 2 // 3, args['epoch'] - 10]:
            for param_group in optimizer.param_groups:
                param_group['lr'] /= 10
        net.train()
        sum_loss, total = 0.0, 0.0
        for i, data in tqdm(enumerate(trainloader, 0)):
            length = len(trainloader)
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)
            outputs, outputs_feature = net(inputs)
            ensemble = sum(outputs[:-1])/len(outputs)
            ensemble.detach_()

            if init is False:
                #   init the adaptation layers.
                #   we add feature adaptation layers here to soften the influence from feature distillation loss
                #   the feature distillation in our conference version :  | f1-f2 | ^ 2
                #   the feature distillation in the final version : |Fully Connected Layer(f1) - f2 | ^ 2
                layer_list = []
                teacher_feature_size = outputs_feature[0].size(1)
                for index in range(1, len(outputs_feature)):
                    student_feature_size = outputs_feature[index].size(1)
                    layer_list.append(nn.Linear(student_feature_size, teacher_feature_size))
                net.adaptation_layers = nn.ModuleList(layer_list)
                net.adaptation_layers.cuda()
                optimizer = optim.AdamW(net.parameters(), lr=args['init_lr'], weight_decay=5e-4)
                #   define the optimizer here again so it will optimize the net.adaptation_layers
                init = True

            #   compute loss
            loss = torch.FloatTensor([0.]).to(device)

            #   for deepest classifier
            loss += criterion(outputs[0], labels)

            teacher_output = outputs[0].detach() # Output after softmax (06/07/2024:QuanDH)
            teacher_feature = outputs_feature[0].detach() # Output after fc layers (06/07/2024:QuanDH)

            #   for shallow classifiers
            for index in range(1, len(outputs)):
                #   logits distillation
                loss += CrossEntropy(outputs[index], teacher_output) * args['loss_coefficient']
                loss += criterion(outputs[index], labels) * (1 - args['loss_coefficient'])
                #   feature distillation
                if index != 1:
                    loss += torch.dist(net.adaptation_layers[index-1](outputs_feature[index]), teacher_feature) * \
                            args['feature_loss_coefficient']
                    #   the feature distillation loss will not be applied to the shallowest classifier

            sum_loss += loss.item()
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total += float(labels.size(0))
            outputs.append(ensemble)



            for classifier_index in range(len(outputs)):
                _, predicted[classifier_index] = torch.max(outputs[classifier_index].data, 1)
                correct[classifier_index] += float(predicted[classifier_index].eq(labels.data).cpu().sum())


        print('[epoch:%d, iter:%d] Loss: %.03f | Acc: 4/4: %.2f%% 3/4: %.2f%% 2/4: %.2f%%  1/4: %.2f%%'
              ' Ensemble: %.2f%%' % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1),
                                      100 * correct[0] / total, 100 * correct[1] / total,
                                      100 * correct[2] / total, 100 * correct[3] / total,
                                      100 * correct[4] / total))

        if use_wandb:
          wandb.log({"epoch":epoch+1,
                    "loss":sum_loss/(i+1),
                    "learning_rate": optimizer.param_groups[0]["lr"],
                    "Acc_4/4":100 * correct[0] / total,
                    "Acc_3/4":100 * correct[1] / total,
                    "Acc_2/4":100 * correct[2] / total,
                    "Acc_1/4":100 * correct[3] / total,
                    "Acc_Ensemble":100 * correct[4] / total
                    })



        print("Waiting Test!")
        all_preds = []
        all_labels = []
        with torch.no_grad():
            correct = [0 for _ in range(5)]
            predicted = [0 for _ in range(5)]
            total = 0.0
            for data in tqdm(testloader):
                net.eval()
                images, labels = data
                images, labels = images.to(device), labels.to(device)
                outputs, outputs_feature = net(images)
                ensemble = sum(outputs) / len(outputs)
                outputs.append(ensemble)
                total += float(labels.size(0))

                for classifier_index in range(len(outputs)):
                    _, predicted[classifier_index] = torch.max(outputs[classifier_index].data, 1)
                    correct[classifier_index] += float(predicted[classifier_index].eq(labels.data).cpu().sum())

                # Collect predictions and labels for confusion matrix of the current best epoch
                if correct[4] / total > best_acc:
                    _, preds = torch.max(ensemble.data, 1)
                    all_preds.extend(preds.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())


            print('Test Set AccuracyAcc: 4/4: %.4f%% 3/4: %.4f%% 2/4: %.4f%%  1/4: %.4f%%'
                  ' Ensemble: %.4f%%' % (100 * correct[0] / total, 100 * correct[1] / total,
                                         100 * correct[2] / total, 100 * correct[3] / total,
                                         100 * correct[4] / total))

            if use_wandb:
              wandb.log({"ValAcc_4/4":100 * correct[0] / total,
                        "ValAcc_3/4":100 * correct[1] / total,
                        "ValAcc_2/4":100 * correct[2] / total,
                        "ValAcc_1/4":100 * correct[3] / total,
                        "ValAcc_Ensemble":100 * correct[4] / total
                        })

            torch.save(net.state_dict(), "./checkpoints/"+str(args['model'])+"-epoch"+str(epoch+1)+str(args['dataset'])+".pth")
            if use_wandb:
              wandb.save("./checkpoints/"+str(args['model'])+"-epoch"+str(epoch+1)+str(args['dataset'])+".pth")
            if epoch > 1:
              os.remove("./checkpoints/"+str(args['model'])+"-epoch"+str(epoch-1)+str(args['dataset'])+".pth")

            if correct[4] / total > best_acc:
                best_acc = correct[4]/total
                print()
                print("Best Accuracy Updated: ", best_acc * 100)
                if use_wandb:
                  wandb.log({"Best Accuracy": best_acc * 100})



        print()

    print("Training Finished, TotalEPOCH=%d, Best Accuracy=%.3f" % (args['epoch'], best_acc))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Compute the confusion matrix
cm = confusion_matrix(all_labels, all_preds)
cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
classes = testset.classes

# Create annotations with percentages
annot = np.empty_like(cm_percentage).astype(str)
for i in range(cm_percentage.shape[0]):
    for j in range(cm_percentage.shape[1]):
        annot[i, j] = f'{cm_percentage[i, j]:.2f}%'

# Plot the confusion matrix
plt.figure(figsize=(12, 10))
sns.heatmap(cm_percentage, annot=annot, fmt='', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted', fontweight='bold')
plt.ylabel('True', fontweight='bold')
plt.title('Confusion Matrix on FERplus', fontweight='bold')
plt.show()